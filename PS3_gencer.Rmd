---
title: "Problem Set 3"
subtitle: "Political Data Science - Spring 2020"
date: "Due February 27, 10:00 AM (Before Class)"
author: "Gencer, Alper Sukru"
output: pdf_document
header-includes:
  - \usepackage{color}
  - \usepackage{float}
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = T, fig.pos = 'H')
```






\section{Instructions}

\bigskip

\begin{enumerate}
  \item The following questions should each be answered within an R script. Be sure to provide many comments
in the script to facilitate grading. Undocumented code will not be graded.
  \item Work on git. Fork the repository found at https://github.com/domlockett/PDS-PS3 and add your
code, committing and pushing frequently. Use meaningful commit messages – these may affect your
grade.
  \item You may work in teams, but each student should develop their own R script. To be clear, there should
be no copy and paste. Each keystroke in the assignment should be your own.
  \item If you have any questions regarding the Problem Set, contact the TAs or use their office hours.
  \item For students new to programming, this may take a while. Get started.
  \item You will need to install ggplot2 and dplyr to complete this dataset.
\end{enumerate}






\newpage
\section{Q1 - ggplot2}

\bigskip

\begin{enumerate}
  \item Finish the exercise we started in class on 2/11/2020:
  \begin{itemize}
    \item Alabama, Arkansas, California, Colorado, Maine, Massachusetts, Minnesota, North Carolina, Oklahoma, Tennessee, Texas, Utah, Vermont, and Virginia will all hold their primraries on March 3
    \item You have been assigned to create a visulaization of the state of the race for this date.
    \item You will make a plot to show this.
    \item In addition to the kinds of issues discussed above
      1- Change to the minimial theme
      2– Figure out how to change the axis labels and legends beyond the defaults
    \item  Visit https://ggplot2.tidyverse.org/reference/
  \end{itemize}
  \item  Finish the exercise we started in class on 2/13/2020:
  \begin{itemize}
    \item Re-organize the dataset so that there is only one row for each candidate-state dyad
    \item Feel free to limit this down to only the relevant candidates
    \item Compare the size of this dataset to our original dataset using the object size command.
  \end{itemize}
\end{enumerate}

\newpage
\section{A1 - ggplot2}

```{r getting ready Q1, include=TRUE, warning = FALSE, message = FALSE}
rm(list = ls())
```

\subsection{Finish the exercise we started in class on 2/11/2020:}

```{r Q1 Part 1, include=TRUE, warning = FALSE, message = FALSE}

####  Let's load the dataset and clean the data:
pP <- primaryPolls <- read.csv('https://jmontgomery.github.io/PDS/Datasets/president_primary_polls_feb2020.csv', stringsAsFactors = F)
pP <- primaryPolls<-primaryPolls[primaryPolls$candidate_name%in%c("Amy Klobuchar", "Bernard Sanders", "Elizabeth Warren", "Joseph R. Biden Jr.", "Michael Bloomberg", "Pete Buttigieg"),]
pP$start_date <- primaryPolls$start_date <-as.Date(primaryPolls$start_date, "%m/%d/%Y")


####  Now let's filter the states we would be doing forecast:
pP.f <- primaryPolls.forcasting<-primaryPolls[primaryPolls$state=="Alabama" | primaryPolls$state=="Arkansas" | primaryPolls$state=="California"| primaryPolls$state=="Colorado"| primaryPolls$state=="Maine"| primaryPolls$state=="Massachusetts"| primaryPolls$state=="Minnesota"| primaryPolls$state=="North Carolina"| primaryPolls$state=="Oklahoma"| primaryPolls$state=="Tennessee"| primaryPolls$state=="Utah"| primaryPolls$state=="Vermont"| primaryPolls$state=="Virginia", ]
unique(primaryPolls.forcasting$state)

####  Now, let's forecast and change the theme to the minimial theme
library("tidyverse")

ggplot(data=pP.f)+
  geom_smooth(mapping = aes(x=start_date, y=pct, color=candidate_name)) +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5)) +
  xlab("Survey Date") +
  ylab("Vote Share (%)") +
  theme(legend.position="bottom") +
  ggtitle("Forecasting of the Candidates in the March 3 Caucus States")

####  Also check the situation in different states

ggplot(data=pP.f)+
  geom_smooth(mapping = aes(x=start_date, y=pct, color=candidate_name)) +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5)) +
  facet_wrap(~ state, nrow=3) +
  xlab("Survey Date") +
  ylab("Vote Share (%)") +
  ggtitle("Forecasting of the Candidates in the March 3 Caucus States")

####  Note that many of these states do not have variation in time. Now, 
# let's look at the states with time variation:

pP.f2 <- pP.f %>%
  filter(state == "California" | state == "Maine" | state == "Massachusetts" | state == "Virginia" )

ggplot(data=pP.f2)+
  geom_smooth(mapping = aes(x=start_date, y=pct, color=candidate_name)) +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5)) +
  facet_wrap(~ state, nrow=1) +
  xlab("Survey Date") +
  ylab("Vote Share (%)") +
  ggtitle("Forecasting of the Candidates in the March 3 Caucus States")
```

It can be seen thatin many of these states the popularity of Warren and Biden is in decline whereas in California, Massachusetts, and Virginia, we see an increasing trend for Bernie Sanders.

\bigskip

\subsection{Finish the exercise we started in class on 2/13/2020:}

```{r Q1 Part 2, include=TRUE, warning = FALSE, message = FALSE}

####  Let's load the dataset and then clean it.
rm(list = ls())
pP <- primaryPolls <- read.csv('https://jmontgomery.github.io/PDS/Datasets/president_primary_polls_feb2020.csv', stringsAsFactors = F)
pP <- primaryPolls<-primaryPolls[primaryPolls$candidate_name%in%c("Amy Klobuchar", "Bernard Sanders", "Elizabeth Warren", "Joseph R. Biden Jr.", "Michael Bloomberg", "Pete Buttigieg"),]
pP$start_date <- primaryPolls$start_date <-as.Date(primaryPolls$start_date, "%m/%d/%Y")


####  Let's re-organize the dataset so that there is only one row for each candidate-state dyad:
library(dplyr)
library(tidyr)
library(readr)

pP.v1 <- pP %>%
  filter(!((state) == "")) %>%
  select(candidate_name, state, pct, start_date)
object.size(pP.v1)

pP.v2 <- pP.v1 %>%
  pivot_wider(names_from = start_date, values_from = pct)
object.size(pP.v2)


print(paste("The size of new dataframe is", round(object.size(pP.v2)/object.size(pP.v1),2),
            "times greater than the previous one."))
```

\newpage
\section{Q2 - tidyverse}

\bigskip

Now you are going to combine two datasets in order to observe how many endorsements each candidate recieved using only dplyr functions. First, create two new objects polls and Endorsements:

```{r Q2 - getting ready, include=TRUE, warning = FALSE, message = FALSE}
rm(list = ls())
library(fivethirtyeight)
library(tidyverse)
polls <- read_csv('https://jmontgomery.github.io/PDS/Datasets/president_primary_polls_feb2020.csv')
Endorsements <- endorsements_2020
```

\begin{itemize}
  \item Change the Endorsements variable name endorsee to candidate name
  \item Change the Endorsements dataframe into a tibble object.
  \item Filter the poll variable to only include the following 6 candidates: Amy Klobuchar, Bernard Sanders,Elizabeth Warren, Joseph R. Biden Jr., Michael Bloomberg, Pete Buttigieg and subset the dataset to the following five variables: $candidate name,\ sample size,\ start date, \ party, \ pct$
  \item Compare the candidate names in the two datasets and find instances where the a candidates name is spelled differently i.e. Bernard vs. Bernie. Using only dplyr functions, make these the same across datasets.
  \item Now combine the two datasets by candidate name using dplyr (there will only be five candidates after joining).
  \item Create a variable which indicates the number of endorsements for each of the five candidates using
dplyr.
  \item Plot the number of endorsement each of the 5 candidates have. Save your plot as an object p.
  \item Run the following code: p + theme dark(). Notice how you can still customize your plot without rerunning the plot with new options. Save this plot in your forked repository
  \item Now, using the knowledge from the last step change the label of the X and Y axes to be more informative, add a title, and use your favorite theme. Save the plot in your forked repository.
\end{itemize}


\newpage
\section{A2 - ggplot2}

\subsection{Change the Endorsements variable name endorsee to candidate name}

```{r Q2 Part 1, include=TRUE, warning = FALSE, message = FALSE}
Endorsements <- Endorsements %>%
  rename(candidate_name = endorsee)
```

\subsection{Change the Endorsements dataframe into a tibble object.}

```{r Q2 Part 2, include=TRUE, warning = FALSE, message = FALSE}
Endorsements <- as_tibble(Endorsements)
```

\subsection{Filter the poll variable to only include the following 6 candidates: Amy Klobuchar, Bernard Sanders,Elizabeth Warren, Joseph R. Biden Jr., Michael Bloomberg, Pete Buttigieg and subset the dataset to the following five variables: candidate_name, sample_size, start_date, party, pct}

```{r Q2 Part 3, include=TRUE, warning = FALSE, message = FALSE}
polls.cand <- polls %>%
  filter(candidate_name == "Amy Klobuchar" | candidate_name == "Amy Klobuchar" | candidate_name ==  "Bernard Sanders" | candidate_name ==  "Elizabeth Warren" | candidate_name ==  "Joseph R. Biden Jr." | candidate_name ==  "Michael Bloomberg" | candidate_name ==  "Pete Buttigieg") %>%
  select(candidate_name, sample_size, start_date, party, pct)
unique(polls.cand$candidate_name)
```

\subsection{Compare the candidate names in the two datasets and find instances where the a candidates name is spelled differently i.e. Bernard vs. Bernie. Using only dplyr functions, make these the same across datasets.}

```{r Q2 Part 4, include=TRUE, warning = FALSE, message = FALSE}
unique(sort(Endorsements$candidate_name))
unique(sort(polls.cand$candidate_name))

####  There are two differences,
# 1-  "Amy Klobuchar" vs "Amy Klobuchar" 
# 2-  "Bernie Sanders" vs "Bernard Sanders"       <-----
# 3-  "Joe Biden" vs "Joseph R. Biden Jr."        <-----
# 4-  "Elizabeth Warren" vs "Elizabeth Warren"
# 5-  "Michael Bloomberg" vs nope
# 6-  "Pete Buttigieg" vs "Pete Buttigieg"  

polls.cand <- polls.cand %>%
  mutate(candidate_name = str_replace(candidate_name, "Bernard Sanders", "Bernie Sanders"))

polls.cand <- polls.cand %>%
  mutate(candidate_name = str_replace(candidate_name, "Joseph R. Biden Jr.", "Joe Biden"))

unique(sort(Endorsements$candidate_name))
unique(sort(polls.cand$candidate_name))

####  COOL!!
```

\subsection{Now combine the two datasets by candidate name using dplyr (there will only be five candidates after joining).}

```{r Q2 Part 5, include=TRUE, warning = FALSE, message = FALSE}

polls.cand <- polls.cand %>%
  group_by(candidate_name) %>%
  mutate(pct.mean = mean(pct))

####  The question says there will be only 5 candidates after merging. Therefore, I assume 
# that we are expected to use 'inner.join'.

inner.merged <- Endorsements %>%
  inner_join(polls.cand, by = 'candidate_name')

unique(inner.merged$candidate_name)

####  Yes, there are five candidates!!

####  However, note that this merging is not a good one as we merged 1000 observations
# into 5000 observations by only candidate names without no other criteria!! 
```

\subsection{Create a variable which indicates the number of endorsements for each of the five candidates using dplyr.}

```{r Q2 Part 6, include=TRUE, warning = FALSE, message = FALSE}
inner.merged <- inner.merged %>%
  group_by(candidate_name) %>%
  mutate(endorse.count = n()) %>%
  arrange(endorse.count)


unique(data.frame(inner.merged$candidate_name, inner.merged$endorse.count))
```

\subsection{Plot the number of endorsement each of the 5 candidates have. Save your plot as an object p.}

```{r Q2 Part 7, include=TRUE, warning = FALSE, message = FALSE, fig.cap = "Candidates by Number of Endorsements - Default Theme", fig.height=3.5}
p <- ggplot(data=inner.merged)+
  geom_point(mapping = aes(x = reorder(candidate_name, endorse.count), y = endorse.count, size = pct.mean)) +
  xlab("Candidate Names") +
  scale_y_continuous(name = "Number of Endorsements", limits=c(0, 30000)) +
  labs( title = "Candidates by their Endorsements", size = "Mean Percentage Vote") +
  theme(legend.position="bottom")

p
```

\subsection{ Run the following code: p + theme_dark(). Notice how you can still customize your plot without rerunning the plot with new options. Save this plot in your forked repository}

```{r Q2 Part 8, include=TRUE, warning = FALSE, message = FALSE, fig.cap = "Candidates by Number of Endorsements - Dark Theme", fig.height=3.5}
p.new <- p + theme_dark() + theme(legend.position="bottom")
p.new
ggsave("candidate_endorsements.pdf")

```

\subsection{Now, using the knowledge from the last step change the label of the X and Y axes to be more informative, add a title, and use your favorite theme. Save the plot in your forked repository.}

```{r Q2 Part 9, include=TRUE, warning = FALSE, message = FALSE, fig.cap = "Candidates by Number of Endorsements - Economist Theme", fig.height=3.5}
####  I have already done with changing labels.
####  Now, I am changing to another theme.

#install.packages("ggthemes")
library(ggthemes)
#?theme_economist

p.new.1 <- p + theme_economist() + theme(legend.position="bottom")
p.new.1
ggsave("candidate_endorsements_my_fav.pdf")

```








\newpage
\section{Q3 - Text-as-Data}

\bigskip

For this assignment you will be analyzing Tweets from President Trump for various characteristics.

```{r Q3 - getting ready, include=TRUE, warning = FALSE, message = FALSE}
rm(list = ls())
library(tidyverse)
#install.packages('tm')
library(tm)
#install.packages('lubridate')
library(lubridate)
#install.packages('wordcloud')
library(wordcloud)
#install.packages("SnowballC")
library(SnowballC)
tweets <- read_csv('https://politicaldatascience.com/PDS/Datasets/trump_tweets.csv')
```

\section{A3 - Text-as-Data}

\subsection{First separate the created_at variable into two new variables where the date and the time are in separate columns. Then report the range of dates that is in this dataset.}

\bigskip

```{r Q3 Part 1, include=TRUE, warning = FALSE, message = FALSE}
tweets.sep <- separate(data = tweets, col = created_at, into = c("date", "time"), sep = " ")
tweets.sep$date<-as.Date(tweets.sep$date, "%m/%d/%Y")
summary(tweets.sep$date)
  
#### As it can be seen here the coverage is ("2014-01-01" to "2020-02-14").
```

\bigskip

\subsection{Using dplyr subset the data to only include original tweets (remove retweets) and show the text of the President’s top 5 most popular and most retweeted tweets. (Hint: The match function can help you find the index once you identify the largest values.)}

\bigskip

```{r Q3 Part 2, include=TRUE, warning = FALSE, message = FALSE}

####  Let's check the most Retweeeted ones:
tweets.pop.retweet <- tweets.sep %>%
  filter(is_retweet == FALSE) %>%
  arrange(desc(retweet_count)) %>%
  select(text) %>%
  slice(1:5)
tweets.pop.retweet

####  Let's check the most favorited ones:
tweets.pop.favorite <- tweets.sep %>%
  filter(is_retweet == FALSE) %>%
  arrange(desc(favorite_count)) %>%
  select(text) %>%
  slice(1:5)
tweets.pop.favorite


```

\bigskip

\subsection{Remove extraneous whitespace, remove numbers and punctuation, convert everything to lower case and remove the standard english stop words and include the following as stop words: c(“see”, “people”,‘new’,‘want’,‘one’,‘even’,‘must’,‘need’,‘done’,‘back’,‘just’,‘going’, ‘know’, ‘can’,‘said’,‘like’,‘many’,‘like’,‘realdonaldtrump’).}

\bigskip

```{r Q3 Part 3, include=TRUE, warning = FALSE, message = FALSE}

####  I exclude his retweets as they do not reflect his words:
####  Convert everything to lower case:
tweets.trump <- tweets.sep %>%
    filter(is_retweet == FALSE) %>%
    mutate(text = str_to_lower(text)) %>%
    select(text)

####  Remove extraneous whitespace
text.as.data.1 <- unlist(str_split(c(unlist(tweets.trump)), " "))

####  Remove numbers and punctuation:
text.as.data.2 <- str_replace_all(text.as.data.1, pattern = "\\d", replacement = "") 
text.as.data.3 <- str_replace_all(text.as.data.2, pattern =  "[^[:alnum:]]", replacement = "") 
text.as.data.4 <- str_replace_all(text.as.data.3, pattern =  "[:digit:]", replacement = "") 

####  Remove the standard english stop words:
stopwords_regex = paste(stopwords('en'), collapse = '\\b|\\b')
stopwords_regex = paste0('\\b', stopwords_regex, '\\b')
text.as.data.5 <- str_replace_all(text.as.data.4, pattern = stopwords_regex, replacement = "") 
extra.stop.words <- c("\\bsee\\b|\\bpeople\\b|\\bnew\\b|\\bwant\\b|\\bone\\b|\\beven\\b|\\bmust\\b|\\bneed\\b|\\bdone\\b|back\\b|\\bjust\\b|\\bgoing\\b|\\bknow\\b|\\bcan\\b|\\bsaid\\b|\\blike\\b|\\bmany\\b|\\blike\\b|\\brealdonaldtrump\\b")
text.as.data.6 <- str_replace_all(text.as.data.5, pattern = extra.stop.words, replacement = "") 
text.as.data <- as_tibble((text.as.data.6)[ text.as.data.6 !=  ""])
```

\bigskip

\subsection{Now create a wordcloud to visualize the top 50 words the President uses in his tweets. Use only words that occur at least three times. Save the plot into your forked repository.}

\bigskip

```{r Q3 Part 4, include=TRUE, warning = FALSE, message = FALSE}

####  Getting number of each tweet
####  Getting rid of dublicates
tweets.pop50words.ready <- text.as.data %>%
  group_by(value) %>%
  mutate(count = n() ) %>%
  distinct() %>%
  arrange(desc(count))

####  Getting first 50 words
tweets.pop50words.ready.2 <- tweets.pop50words.ready[1:50,]

wc <- wordcloud(words = tweets.pop50words.ready.2$value, freq = tweets.pop50words.ready.2$count, min.freq = 3,
          max.words=, random.order=FALSE, rot.per=0.35, 
          colors=brewer.pal(8, "Dark2"))
```

\bigskip

\subsection{Create a document term matrix called DTM that includes the argument control = list(weighting = weightTfIdf)}

\bigskip

```{r Q3 Part 5, include=TRUE, warning = FALSE, message = FALSE}
#install.packages("corpus")
library("corpus")

x <- VectorSource(tweets.sep$text)
x <- VCorpus(x)
DTM <- DocumentTermMatrix(x, control = list(weighting = weightTfIdf))
```

\bigskip

\subsection{Finally, report the 50 words with the the highest tf.idf scores using a lower frequency bound of .8.}


\bigskip

```{r Q3 Part 6, include=TRUE, warning = FALSE, message = FALSE}
findFreqTerms(DTM, lowfreq = 0.8)

####  At this point, I am not sure how to move as I coult not order the words by their frequency :/
```

\bigskip

